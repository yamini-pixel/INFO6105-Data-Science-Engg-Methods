{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Take-home Midterm</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 14 March 2023</div>\n",
    "\n",
    "# So, is it the car or is it the driver, in ten parts\n",
    "\n",
    "Well, I got this idea for your midterm when one of you asked the question in class \"*so how do we model F1 with a Bayesian approach*\"?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/f1-2022.jpg\" width=1000 />\n",
    "</center>\n",
    "\n",
    "The big question in F1 is [is it the car or is it the driver](https://www.total-motorsport.com/f1-car-or-driver-more-important/). We want to begin to answer this question with modern bayesian statistics.\n",
    "\n",
    "We want to infer the hidden parameters (how talented a F1 driver is, how strong a F1 constructor is - these parameters are hidden to us) that are driving the data we observe (points), and be able to use these to predict ***future*** points by building a **model** for each car.\n",
    "\n",
    "We want to start with some **prior** distributions, and use observations (past results) to refine our priors into **posteriors**. Points are a noisy measurement of car strength, so quantifying car strength **uncertainty** is important. It will help us determine where to bet less (cars we're weaker at predicting), and cars to bet more on (cars where uncertainty is minimal).\n",
    "\n",
    "We will use a Poisson count model to model F1 car points, since [F1 points](https://en.wikipedia.org/wiki/Formula_One_racing) awarded, exclusively for positions 1 through 10, are integer points.\n",
    "\n",
    ">*Points are awarded to drivers and teams based on where they finish in a race. The winner receives 25 points, the second-place finisher 18 points, with 15, 12, 10, 8, 6, 4, 2 and 1 points for positions 3 through 10, respectively*.\n",
    "\n",
    "I did a bit of litterature research and found this interesting [paper](https://arxiv.org/pdf/2203.08489.pdf), which underscores that in a race, it's mostly about the car, rather than the driver.\n",
    "\n",
    ">**Note**: We will simplify the paper's approach somewhat, but if you read the paper carefully, you will realize what a long trip we have taken together already and how much you've learned. This is an advanced research paper in data science, so if you feel somewhat comfortable reading it, well done student!\n",
    "\n",
    "Let's see if this is true.\n",
    "\n",
    "First, a bit about pymc3...\n",
    "\n",
    "# About pymc3\n",
    "`PyMC3` has been renamed to `PyMC`. PyMC3 version 3.x will stay under the current name to not break production systems\n",
    "\n",
    "Here's what to remember:\n",
    "\n",
    "- PyMC3 3.x: The version we all love and use, using the `theano-pymc` tensor\n",
    "- PyMC version 4, with the latest version being 5.1.1: The successor to PyMC3, (mostly) API compatible, and main focus going forward.\n",
    "- (PyMC4: A discontinued experiment with new API on the TensorFlow backend).\n",
    "\n",
    "`Theano` is a computational graph library, compiling complex mathemtical operations into one graph that can be optimized (we experimented with similar optimizations when we contrasted lazy list comprehensions and generators versus eager list comprehensions and functions). Theano was absorbed into tensorflow, at which point the Theano authors (Montral university) discontinued it. The pymc3 authors then did what everyone would do: Upgrade pymc3 to use tensorflow, but they failed (thus discontinued `pymc4`). After rediscovering the power of Theano, the pymc3 authors forked Theano to `Theano-PyMC` (PyMC3 now relies on this fork), and then reforked to `aesara`, and then reforked to `pytensor`.\n",
    "\n",
    "In v3, pymc3 was not really using the full power of Theano. While calling e.g. `x = pm.Normal('x')` created a `theano.TensorVariable` which can be manipulated further, like inputting it into another random variable, a full-fledged graph was not being built.\n",
    "\n",
    "[Here's](https://gist.github.com/twiecki/0219d3fa059776b90af0f08b5452e346) more detail about all these shenanigans.\n",
    "\n",
    "If you can run pymc3 with the theano tensor on your laptop (run first cell in this notebook), great, keep using it. Otherwise, please create a new environment and install pymc version 5.1.1 based on python 3.10:\n",
    "```\n",
    "conda install nb_conda_kernels\n",
    "conda create -n pymc5 python=3.10\n",
    "conda activate pymc5\n",
    "conda install -c conda-forge pymc==5.1.1\n",
    "pip install seaborn\n",
    "conda install ipykernel\n",
    "(restart anaconda kernel)\n",
    "```\n",
    "\n",
    "then open the notebook, go to kernels menu, and switch to your new environment by selecting the pymc5 kernel, and run the second cell in this notebook instead of the first cell. ***DO NOT*** run both cells. Pick the one you will use.\n",
    "\n",
    "Run the cell below if you're using the `theano` tensor. If it works, keep using the theano tensor and ***do not run the second cell below***.\n",
    "\n",
    "If you don't have `theano`, another option would be to stick with `pymc3` and replace `tt.exp` with `pm.math.exp` and `tt.mean` with `pm.math.sum(..)/num_teams` in setting up the simulation. So, you have a number of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cachetools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrMethodFormatter\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\__init__.py:49\u001b[0m\n\u001b[0;32m     44\u001b[0m     pytensor\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgcc__cxxflags \u001b[38;5;241m=\u001b[39m augmented\n\u001b[0;32m     47\u001b[0m __set_compiler_flags()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _version, gp, ode, sampling\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\gp\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#   Copyright 2023 The PyMC Developers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#   Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#   limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cov, mean, util\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     TP,\n\u001b[0;32m     18\u001b[0m     Latent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     MarginalSparse,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\gp\\util.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kmeans\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Avoid circular dependency when importing modelcontext\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distribution\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modelcontext\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytensorf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_pymc, walk_model\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\distributions\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#   Copyright 2023 The PyMC Developers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#   Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#   limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbound\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bound\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcensored\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Censored\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     AsymmetricLaplace,\n\u001b[0;32m     19\u001b[0m     Beta,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     Weibull,\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\distributions\\bound.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomVariable\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorVariable\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoundedContinuous, bounded_cont_transform\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdist_math\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_parameters\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Continuous, Discrete\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\distributions\\continuous.py:80\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expit\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdist_math\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m     SplineWrapper,\n\u001b[0;32m     82\u001b[0m     check_parameters,\n\u001b[0;32m     83\u001b[0m     clipped_beta_rvs,\n\u001b[0;32m     84\u001b[0m     i0e,\n\u001b[0;32m     85\u001b[0m     log_normal,\n\u001b[0;32m     86\u001b[0m     logpow,\n\u001b[0;32m     87\u001b[0m     normal_lccdf,\n\u001b[0;32m     88\u001b[0m     normal_lcdf,\n\u001b[0;32m     89\u001b[0m     zvalue,\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DIST_PARAMETER_TYPES, Continuous\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rv_size_is_none\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\distributions\\dist_math.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melemwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elemwise\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mslinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cholesky, SolveTriangular\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_tuple\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogprob\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckParameterValue\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytensorf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floatX\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\distributions\\shape_utils.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorVariable\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeAlias\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modelcontext\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytensorf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_observed_data\n\u001b[0;32m     40\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_broadcasting\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange_dist_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m ]\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\model.py:62\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _default_transform\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     BlockModelAccessError,\n\u001b[0;32m     57\u001b[0m     ImputationWarning,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     ShapeWarning,\n\u001b[0;32m     61\u001b[0m )\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitial_point\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_initial_point_fn\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogprob\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjoint_logprob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joint_logp\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytensorf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     PointFunc,\n\u001b[0;32m     66\u001b[0m     SeedSequenceSeed,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     replace_rvs_by_values,\n\u001b[0;32m     73\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\initial_point.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogprob\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RVTransform\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytensorf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_pymc, find_rng_nodes, replace_rng_nodes, reseed_rngs\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_transformed_name, get_untransformed_name, is_transformed_name\n\u001b[0;32m     31\u001b[0m StartDict \u001b[38;5;241m=\u001b[39m Dict[Union[Variable, \u001b[38;5;28mstr\u001b[39m], Union[np\u001b[38;5;241m.\u001b[39mndarray, Variable, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[0;32m     32\u001b[0m PointType \u001b[38;5;241m=\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\class\\lib\\site-packages\\pymc\\util.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcachetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRUCache, cachedmethod\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SharedVariable\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cachetools'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "import pymc as pm, theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cell above failed, then you should probably be using `pytensor` instead of `theano`.\n",
    "```\n",
    "conda install nb_conda_kernels\n",
    "conda create -n pymc5 python=3.10\n",
    "conda activate pymc5\n",
    "pip install pymc==5.1.1\n",
    "pip install seaborn\n",
    "conda install ipykernel\n",
    "(restart anaconda kernel)\n",
    "```\n",
    "\n",
    "Then run the cell below, and keep on truckin'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "import pymc3 as pm\n",
    "\n",
    "# import pytensor\n",
    "# from pytensor import tensor as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Note***: If you get the following output, then you're probably hosed. See your TAs!\n",
    "```\n",
    "WARN: Could not locate executable g77\n",
    "WARN: Could not locate executable f77\n",
    "WARN: Could not locate executable ifort\n",
    "WARN: Could not locate executable ifl\n",
    "WARN: Could not locate executable f90\n",
    "WARN: Could not locate executable efl\n",
    "WARN: Could not locate executable gfortran\n",
    "WARN: Could not locate executable f95\n",
    "WARN: Could not locate executable g95\n",
    "WARN: Could not locate executable efort\n",
    "WARN: Could not locate executable efc\n",
    "WARN: Could not locate executable flang\n",
    "WARN: don't know how to compile Fortran code on platform 'nt'\n",
    "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
    "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The data for F1 season 2022\n",
    "I found it [here](https://github.com/toUpperCase78/formula1-datasets)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_races = pd.read_csv('data/Formula1_2022season_raceResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "Do some data healing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Points\n",
    "Build a dictionary that represents the race points won by a driver as a function of race finishing position.\n",
    "\n",
    "Then, build a pandas dataframe called `df_races_positions` with the same index as the `df_races` dataframe, but with only the following columns: `Position`, `Driver`, `Team`, `Points`.\n",
    "\n",
    "Then, build the following dataframes (in full) representing driver + team and total points for the season, and team and points for the season:\n",
    "```\n",
    "\tPosition\tPoints\n",
    "Driver\tTeam\t\t\n",
    "Max Verstappen\tRed Bull Racing RBPT\t59\t428\n",
    "Charles Leclerc\tFerrari\t63\t288\n",
    "Sergio Perez\tRed Bull Racing RBPT\t79\t288\n",
    "George Russell\tMercedes\t92\t259\n",
    "Lewis Hamilton\tMercedes\t113\t231\n",
    "...\n",
    "```\n",
    "\n",
    "```\n",
    "Position\tPoints\n",
    "Team\t\t\n",
    "Red Bull Racing RBPT\t138\t716\n",
    "Ferrari\t119\t514\n",
    "Mercedes\t205\t490\n",
    "Alpine Renault\t311\t170\n",
    "...\n",
    "```\n",
    "\n",
    "Then look at the mean and variance of driver points. You should find that mean and variance are not super different in range, so it makes sense to use a Poisson distribution to model Points.\n",
    "\n",
    "Plot a bar chart with Season 2022 Standings for each driver.\n",
    "\n",
    "Index the drivers and the teams, and build a python list called `drivers_observed` that lists all the drivers by index in the order of the `df_races` dataframe.\n",
    "\n",
    "Do the same with Teams (also called *constructors*) in a python list called `constructors_observed`.\n",
    "\n",
    "Finally, build a python list called `points_observed` that represents the points awarded to each car finishing in the order of the `df_races` dataframe.\n",
    "\n",
    "These three lists will figure prominently in our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_races' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m race_points_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m7\u001b[39m: \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m9\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new dataframe with only the relevant columns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_races_positions \u001b[38;5;241m=\u001b[39m \u001b[43mdf_races\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDriver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Add a new column to the dataframe that maps finishing position to race points\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_races_positions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoints\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_races_positions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(race_points_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_races' is not defined"
     ]
    }
   ],
   "source": [
    "# Create dictionary to map race finishing position to race points\n",
    "race_points_dict = {1: 25, 2: 18, 3: 15, 4: 12, 5: 10, 6: 8, 7: 6, 8: 4, 9: 2, 10: 1}\n",
    "\n",
    "# Create a new dataframe with only the relevant columns\n",
    "df_races_positions = df_races[['Position', 'Driver', 'Team']]\n",
    "\n",
    "# Add a new column to the dataframe that maps finishing position to race points\n",
    "df_races_positions['Points'] = df_races_positions['Position'].map(race_points_dict)\n",
    "\n",
    "# Create a pivot table to aggregate points by driver and team\n",
    "df_driver_team_points = pd.pivot_table(df_races_positions, values='Points', index=['Driver', 'Team'], aggfunc=np.sum)\n",
    "\n",
    "# Create a pivot table to aggregate points by team\n",
    "df_team_points = pd.pivot_table(df_races_positions, values='Points', index=['Team'], aggfunc=np.sum)\n",
    "\n",
    "# Sort the pivot tables by points\n",
    "df_driver_team_points = df_driver_team_points.sort_values(by=['Points'], ascending=False)\n",
    "df_team_points = df_team_points.sort_values(by=['Points'], ascending=False)\n",
    "\n",
    "# Calculate the mean and variance of driver points\n",
    "driver_points_mean = df_driver_team_points['Points'].mean()\n",
    "driver_points_variance = df_driver_team_points['Points'].var()\n",
    "\n",
    "# Plot a bar chart with Season 2022 Standings for each driver\n",
    "df_driver_team_points.plot(kind='bar', y='Points', legend=False)\n",
    "plt.ylabel('Points')\n",
    "plt.title('Season 2022 Standings')\n",
    "plt.show()\n",
    "\n",
    "# Get a list of all observed drivers and constructors\n",
    "drivers_observed = df_races_positions['Driver'].unique()\n",
    "constructors_observed = df_races_positions['Team'].unique()\n",
    "\n",
    "# Get a list of points awarded to each car finishing in the order of the df_races dataframe\n",
    "points_observed = df_races_positions['Points'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bayesian Power Model\n",
    "\n",
    "We know that a **count outcome** is modelled as a **Poisson distribution**, unless mean and standard deviation really diverge, in which case we use the **negative binomial**. \n",
    "\n",
    "Modeling car strength is usually accomplished with a vector of points scored $y$ as a Poisson distributions: $(y_i\\;|\\;θ_j) \\propto Poisson(θ_j)$ where the $\\theta_j$ parameters represent the Poisson **expectation** for the car finishing the race.\n",
    "\n",
    "We will follow the approach of the research paper, simplifying it a bit, and model the point expectation $\\theta$ of each car as an exponential of *two* independent variables: a drivers variable **drv**, and a constructor variable **csr**, times a constant factor k:\n",
    "\n",
    "$$θ_c = k_c * e^{\\text{drv}_c} * e^{\\text{csr}_c}$$\n",
    "\n",
    "Instead of 4 (in the paper) where the other two parameters are longer-term effects related to the *experience* of each team *across* seasons. \n",
    "\n",
    ">**Note**: Exponential, professor? I thought we were building **linear models**!\n",
    "\n",
    "Yes, we use an exponential to increase the effect of each indendent variable,  but we will use $log(\\theta)$ to simplify to a linear model!\n",
    "\n",
    "Parameters are initially modeled with a log-linear random effect model, a standard procedure in sports analytics.\n",
    "\n",
    "$$\\log(θ) = log(k) + \\text{drv}_c + \\text{csr}_c $$\n",
    "\n",
    ">**Note**: Wait, where's the $x$? There really ***is no $x$***! So we don't really have a linear model, here. Instead, we have an **additive model**, and we create two *new* columns: $\\text{att}_g$ and $\\text{def}_g$.\n",
    "\n",
    "Let's assume results are determined jointly by a ***driver*** and ***constructor*** ability for each event, an aggregation of the talent of drivers and mechanics, represented by parameters `drv` and `csr`, respectively.\n",
    "\n",
    "For each car race result $c = 1, \\cdots, C$, car-specific `drv` and `csr` effects will be modelled by a common normal distribution:\n",
    "$drv_c \\propto \\text{Normal}(μ_{drv},τ_{drv})$ and $csr_c \\propto \\text{Normal}(μ_{csr}, τ_{csr})$.\n",
    "\n",
    "We will initialize $μ_{drv}$ and $μ_{csr}$ to 0, which is a common action in mcmc sims for modeling likelihood parameter pdfs.\n",
    "\n",
    "As for $\\sigma_{drv}$ and $\\sigma_{csr}$, we will initialize them to... *another pdf*! This gives the mcmc engine more leeway in the simulation. We will initialize them to half-Student-t distributions. The half-Student-t parameters will be set to match the data's.\n",
    "\n",
    "We will build our model in `PyMC3`, specifying global parameters, car-specific parameters, and the likelihood function.\n",
    "\n",
    "Ok, so the data we want to consider are the points awarded for each car in `df_races`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Priors\n",
    "The number of points scored for the entire season, divided by the number of races is a good indicator of both ***driver talent*** and ***constructor know-how***.\n",
    "\n",
    "So, these will be our priors for the `drv` and `csr` parameters for each car: \n",
    "\n",
    "Take the floor of the mean number of points for 2022 per driver, and then its logarithm. It should look like this:\n",
    "```\n",
    "Driver\n",
    "Max Verstappen      6.059123\n",
    "Charles Leclerc     5.662960\n",
    "Sergio Perez        5.662960\n",
    "....\n",
    "```\n",
    "\n",
    "Replace infinities with zeros.\n",
    "\n",
    "Build a python list called `drv_starting_points` that represents these values, in the order of your driver index.\n",
    "\n",
    "Take the floor of the mean number of points for 2022 per constructor, and then its logarithm. \n",
    "\n",
    "Build a python list called `csr_starting_points` that represents these values, in the order of your team index.\n",
    "\n",
    "These will be our priors for our sim,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood function and model parameters\n",
    "Now let's specity our **likelihood function** in `pymc3`. \n",
    "\n",
    "A bit of theory:\n",
    "\n",
    "You should know that whenever we have a small number of data point (about 30 or less), the [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) gives better results than the normal distribution. \n",
    "\n",
    "If you are modeling the distribution of a *large* population, then use the **normal distribution**. If you are modeling the mean of the population when the population size is small, better to use the **student-t distribution**.\n",
    "\n",
    "The student-t distribution has higher kurtosis (4th *moment*) than the normal distribution. That means that data from the t-distribution will have a tendency to appear closer or farther from the mean (it's like it says on your rear-view mirror: ***objects are closer than they appear***) than typical normal data, with a more sudden transition in between. In other words, the t-distribution is more *fat-tailed* and captures outliers ***better***: The probability of obtaining values far from the mean is larger than with the normal distribution.\n",
    "\n",
    "As the number of degrees of freedom of data ($\\nu$) increases, the t-distribution becomes closer to the normal distribution. The t-distribution is used a lot in [fintech](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-7701-4_18). Financial data does tend to have higher kurtosis than the normal distribution, and exhibits wild jumps up or down  more frequently than what the normal distribution predicts.\n",
    "\n",
    "The **half-t distribution** (also denoted **folded-t**) is derived from the t-distribution by taking the absolute values of variates. Since we have positive points, we don't want any negative signs.\n",
    "\n",
    "But we're not going to use neither the gaussian nor the Student-t as our data likelihood. We are going to use the Poisson distribution, because we have *count* data.\n",
    "\n",
    "Since we decided on a linear model for log$\\theta$, we usually have a *slope-intercept* formula: $y = log(\\theta) = mx + b$ where $m$ is the slope or the consistent change between $x$ and $y = log(\\theta)$, and $b$ is the y-intercept (intercepts the y-axis at $y = b$). It is the expected mean value of $y$ when $x = 0$. Now, what is $x$? $x$ is the keys, and here we have discrete keys: drivers and constructors. Linear models are most often used with continuous keys. Here, since we have discrete keys, instead of a linear model we are going to use an **additive model**, where $log(\\theta)$ can be expressed as the sum of two variables, each modeled as a stochastic distribution.\n",
    "\n",
    "So here's the model and likelihood in `pymc3`, starting from the bottom of the cell below, to the top:\n",
    "\n",
    "- The points scored by cars, our **data likelihoods**, will be modelled by Poisson distributions. The Poisson distribution has a single parameter: its **Expectation**, which we will model with a **bivariate linear model** with the two features being ***driver talent*** and ***constructor talent***. \n",
    "\n",
    "- Actually, we will compute with the **log** of the expectation, so to make the log of the expectation a linear model, we will assume that the expectation is actually an **exponential**, with an additive model for the exponent.\n",
    "\n",
    "- Since driver talent and constructor know-how are relative concepts, we will model these strengths as ***car-specific strengths minus the average across all cars***. Note that we are using an additive relational. We also could have used a multiplicative relation whereby talents and know-how are the *ratio* of car-specific values, normalized by the average across all cars.\n",
    "\n",
    "- The car-specific driver and constructor strenghts (talent and know-how) are parameters of our data likelihood, so we need to model them as probability density functions. What to pick? When we don't know, we pick **normal distributions**! How about their mean and standard deviation?\n",
    "\n",
    "- Bayesian estimation usually assumes a 0 mean for these parameters. For the standard deviation (noise) we will use a half-normal. But to account for many possible outliers (liek Ferrari in 2022 that made a ton of [strategic mistakes](https://www.gpblog.com/en/news/163001/numerous-mistakes-by-ferrari-in-2022-show-it-was-time-for-binotto-exit.html)), we will use a **half-student-T** instead (noise is never negative, that's why a **half** profile). The student-T has 3 parameters itself, and here we select numbers: Pure guesses (mean 0, sd of 2.5, and a nu of 3).\n",
    "\n",
    "- The intercept is completely unknown, so we select a **uniform distribution** (all values equally likely)\n",
    "\n",
    "If you have no idea about the intercept, you could model it with a uniform distribution:\n",
    "```\n",
    "    intercept = pm.Flat('intercept') #flat pdf is uninformative - means we have no idea\n",
    "```\n",
    "\n",
    "Your car-specific model parameters, specifically one `(drvs_star, csrs_star)` tuple per car, could be modeled with normal distributions (you will need to replace the `?`s. Hint: use your priors!):\n",
    "```\n",
    "    drvs_star = pm.Normal(\"drvs_star\", mu=0, sd=?, shape=?)\n",
    "    csrs_star = pm.Normal(\"csrs_star\", mu=0, sd=?, shape=?)\n",
    "```\n",
    "\n",
    "To allow samples of expressions to be saved, we need to wrap them in pymc3 Deterministic objects. Since we only care about *relative* strengths, we subtract from `drvs_star` and `csrs_star` the mean across drivers and constructors. We save these parameters for later exploration:\n",
    "```\n",
    "    drvs = pm.Deterministic('drvs', drvs_star - tt.mean(drvs_star))\n",
    "    csrs = pm.Deterministic('csrs', csrs_star - tt.mean(csrs_star))\n",
    "```\n",
    "\n",
    "Assume an exponential search on the theta parameter:\n",
    "```\n",
    "    theta = tt.exp(intercept + drvs[drivers_observed] + csrs[constructors_observed])  \n",
    "```\n",
    "\n",
    "Assume a Poisson likelihood on the observed data (Points) with expectation `theta`:\n",
    "```\n",
    "    points = pm.Poisson('points', mu=theta, observed=points_observed)\n",
    "```\n",
    "\n",
    "Note that if you don't have `theano`, you could replace `tt.exp` with `pm.math.exp` and `tt.mean` with `pm.math.sum(..)/num_teams`. \n",
    "\n",
    "If you opted for pymc 5.1.1, you already have `pytensor` installed and you can replace `theano` with `pytensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Simulation\n",
    "Run the mcmc simulation and plot the traces for your model parameters (You may have to replace `pm.traceplot()` with `pm.plot_trace`).\n",
    "\n",
    "```\n",
    "    trace = pm.sample(5000, tune=1000, cores=1)\n",
    "    pm.traceplot(trace)\n",
    "```\n",
    "\n",
    "Then, tabulate your model parameters:\n",
    "```\n",
    "import arviz as az\n",
    "az.summary(trace, round_to=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Setting a hypothetical constraint\n",
    "Let's set a constraint that Charles Leclerc (Ferrari) is a better driver than Max Verstappen (Red Bull). How much better does The Red Bull constructor need to be in order to justify this constraint?\n",
    "\n",
    "Provide this answer in a percentage relative to Ferrari and Mercedes.\n",
    "\n",
    "A `pm.Potential()` is an arbitrary factor that you can add to the model likelihood. In this example, by using `tt.switch()`, if the parameters satisfies you constraints in `tt.switch`, we add nothing to the model, otherwise we add `-inf` to the log-likelihood of the model, making it *impossible*.\n",
    "\n",
    "Let's set a constraint that Charles Leclerc (Ferrari) is a better driver than Max Verstappen (Red Bull)\n",
    "```\n",
    "    pm.Potential('CL_>_MV', tt.switch( tt.mean(drvs_star[1]) > tt.mean(drvs_star[0]), 0., -np.inf))\n",
    "```\n",
    "\n",
    "You may replace the `tt.switch` API on setting a simulation constraint to `pm.math.switch`.\n",
    "\n",
    "Then, run the sim again. Note that you may have to limit it to 1 chain due to numerical instabilities (simulation blows up with `Bad initial energy`). You will get a *lot* of divergences with the constraint, representing cases where the constraint is *not* fulfilled and thus sampled off.\n",
    "```\n",
    "`trace = pm.sample(5000, tune=1000, cores=1, chains=1)\n",
    "```\n",
    "\n",
    "Conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plots\n",
    "\n",
    "From model posteriors, plot driver talent *and* constructor strength with *and* without the constraint above, *with* **credible intervals**, so you have uncertainty quantification of your estimate of cars' strengths.\n",
    "\n",
    "For example, your plot for driver talent should look like this (I removed the names of the drivers on the x-axis):\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/driver_strength_unknown.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "In Bayesian statistics, a **credible interval** is a range of values within which an unobserved parameter value falls with a particular subjective probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution.\n",
    "\n",
    "Credible intervals are analogous to **confidence intervals** in frequentist statistics we studied when we studied the t-test?\n",
    "\n",
    "Bayesian intervals treat bounds as *fixed* and the estimated parameter as a *random variable*. In our experiment to determine the distribution of team strength `att`, if the subjective probability that `att`$_{germany}$ lies between 0.35 and 0.55 is 0.95, then $0.35\\leq \\text{att} \\leq 0.55$ is a 95% credible interval.\n",
    "\n",
    "Frequentist confidence intervals treat bounds as random variables and the parameter as a fixed value. A frequentist 95% confidence interval means that with a large number of repeated samples, 95% of such calculated confidence intervals would include the true value of the parameter. Aren't Bayesian credible intervals a much better metric? I think they are. However, Bayesian credible intervals do require knowledge of a situation-specific prior distribution, while frequentist confidence intervals do not.\n",
    "\n",
    "The **highest posterior density interval** (HDI) is the interval which contains the required point estimate such that all points within the interval have a higher probability density than points outside the interval. The HDI is the narrowest interval containing the specified point estimate. Locating the HDI is usually accomplished using the Chen-Shao algorithm (Chen and Shao; 1999; Chen, Shao, and Ibrahim; 2000). For more info, this is the best [reference](https://cran.r-project.org/web/packages/HDInterval/HDInterval.pdf) i've found. In `pymc3`, you get it with `pm.stats.hpd`.\n",
    "\n",
    "**Quantiles** are sets of values of a variate that divide a frequency distribution into equal groups, each containing the same fraction of the total population. The *Median* is an example of a quantile that separates the two halves of a group.\n",
    "\n",
    "Then, plot posterior pdfs for all drivers and for all constructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about Red Bull Racing\n",
    "[Red Bull Racing](https://en.wikipedia.org/wiki/Red_Bull_Racing), also simply known as Red Bull or RBR and currently competing as Oracle Red Bull Racing, is one of two Formula One teams owned by conglomerate Austrian company Red Bull GmbH, the other being Scuderia AlphaTauri (previously Scuderia Toro Rosso).\n",
    "\n",
    "Red Bull had **Cosworth** engines in 2005 and **Ferrari** engines in 2006. \n",
    "\n",
    "The team used engines supplied by **Renault** between 2007 and 2018 (from 2016 to 2018, the Renault engine was re-badged \"TAG Heuer\" following the breakdown in the relationship between Red Bull and Renault in 2015). During this partnership, they won four successive Drivers' and Constructors' Championship titles from 2010 to 2013, becoming the first Austrian team to win the title. \n",
    "\n",
    "The team began using **Honda** engines in 2019. The works Honda partnership culminated in 2021 following Red Bull driver Max Verstappen's World Drivers' Championship victory, with Verstappen also winning the championship in 2022. Honda left the sport officially after 2021, but will continue to supply complete engines from Japan to the team under Red Bull Powertrains branding until the end of 2025.\n",
    "\n",
    "Starting in 2025, **Ford** will supply the engine, starting with a hybrid (gas/battery) block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Validating your model with Posterior Predictive Checks\n",
    "\n",
    "Comparing the data to **posterior predictions** of the model is called a **posterior predictive check** (PPC). When there appear to be systematic discrepancies that would be meaningful to address, we should consider expanding or changing the model so it may be a better description of the data.\n",
    "\n",
    "You can read [here](https://www.sciencedirect.com/topics/mathematics/posterior-predictive-check) about Posterior Predictive Checks, and [here](https://www.sciencedirect.com/topics/mathematics/bayesian-model-comparison) about the sensitivity of the Bayes Factor to priors, whenc comparing different models (for example, one model that predicts planetary motions based on elliptical orbits around the sun, as posited by Galileo Galilei, and another that predicts planetary motions based on circular cycles and epicycles around the earth, as posited by the Catholic Church).\n",
    "\n",
    "Let's do **posterior predictive checks**  in order to validate our model. The idea is to generate data from the model using parameters from draws from the posterior.\n",
    "\n",
    "PPCs can help analyze the degree to which data generated from the model deviate from data generated from the true distribution. Visualizing the PPC is a great \"***sense check***\" of your model. In `pymc3`, it's done with `sample_ppc()` of your trace.\n",
    "\n",
    "Posterior predictive checks are just like a simulation, with the optimal parameters of your model. \n",
    "\n",
    "It's like generating new futures from your model, so that you can generate, let's say 100 futures to see what the mean outcome will be.\n",
    "\n",
    "For example: `ppc = pm.sample_ppc(trace, samples=500, model=f1_model, size=100)` will randomly draw 500 samples of parameters from the trace. Then, for each sample, it will draw 100 random variates from the Poisson distribution specified by the values of the parameters in that sample. \n",
    "\n",
    "If one does not specify the `samples` and `size` parameters, then it will draw one random variate from each trace point:\n",
    "```\n",
    "pp_trace = pm.sample_posterior_predictive(trace)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that your original dataset had 440 observations.\n",
    "\n",
    "So, with your model, generate 10,000 times season 2022 of F1. The observations order for each one of these simulated season is the same as our original data.\n",
    "\n",
    "Gather all the simulated points and plot the HPD of Driver talent and constructor know-how after 10,000 simulated seasons. Verify that it matches your empirical results.\n",
    "\n",
    "This is how you can verify that your model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means our model produces results *statistically equivalent* to our observations. \n",
    "\n",
    "This verifies that our model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The world where CL > MV\n",
    "In this model, we used a constraint to force Charles Leclerc's talent to be higher than Max Verstappen. \n",
    "\n",
    "Generate a posterior predictive distribution to see who wins the season in this world. Keep in mind that in this world, Red Bull racing should have been found to be rated *much higher* than competitors in constructor know-how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the hypothetical model where Charles Leclerc is a more talented driver than Max Verstappen still *possible*? Are posterior checks producing results in line with observations? Can you reject this model as an unlikely one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Probability that Ferrari wins two consecutive races\n",
    "Since we can produce simulated data that potentially covers all possible situations, it is straightforward now to extract conditional probabilities.\n",
    "\n",
    "We need tons of data to cover edge cases, so let's produce *a million* simulated 2022 seasons.\n",
    "```\n",
    "pp_trace_f = pm.sample_ppc(trace, samples=1000000, model=...)\n",
    "```\n",
    "\n",
    "Then, compute the probability that Ferrari comes in 1/2 in any race in the 2022 season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find it to be very small.\n",
    "\n",
    "Indeed, Ferrari did not finish 1/2 in 2022.\n",
    "\n",
    "But Mercedes and Red Bull [did](https://www.lightsoutblog.com/2022/11/16/the-last-1-2-finish-for-every-team-on-the-f1-grid/)!\n",
    "\n",
    "This is another example of our universe producing stranger results than expected (and why we need the student-t distribution!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
